---
title: Trabajo Análisis Exploratorio de Datos
author:
  - name: Luis Miguel Rioja Gallo, Anna Cabrerizo Requena
    affil: 1,2,\ddagger,*
    orcid: 0000-0003-3293-2315
  - name: John Doe
    affil: 2, \dagger, \ddagger
affiliation:
  - num: 1
    address: |
      Universitat de València - 
      Etse. Burjassot, València
    email: ancare2@alumne.uv.es
  - num: 2
    address: |
      Your department
      Street, City, Country
    email: mail@mail.com
# author citation list in chicago format
authorcitation: |
  Leutnant, D.; Doe, J.
# firstnote to eighthnote
firstnote: |
  Current address: Updated affiliation
secondnote: |
  These authors contributed equally to this work.
correspondence: |
  leutnant@fh-muenster.de; Tel.: +XX-000-00-0000.
# document options
journal: notspecified
type: article
status: submit
# front matter
simplesummary: |
  A Simple summary goes here.
abstract: |
  A single paragraph of about 200 words maximum. For research articles, 
  abstracts should give a pertinent overview of the work. We strongly encourage
  authors to use the following style of structured abstracts, but without 
  headings: 1) Background: Place the question addressed in a broad context and
  highlight the purpose of the study; 2) Methods: Describe briefly the main
  methods or treatments applied; 3) Results: Summarize the article's main 
  findings; and 4) Conclusion: Indicate the main conclusions or interpretations. 
  The abstract should be an objective representation of the article, it must not 
  contain results which are not presented and substantiated in the main text and 
  should not exaggerate the main conclusions.
# back matter
keywords: |
  keyword 1; keyword 2; keyword 3 (list three to ten pertinent keywords specific 
  to the article, yet reasonably common within the subject discipline.).
acknowledgement: |
  All sources of funding of the study should be disclosed. Please clearly 
  indicate grants that you have received in support of your research work. 
  Clearly state if you received funds for covering the costs to publish in open 
  access.
authorcontributions: |
  For research articles with several authors, a short paragraph specifying their 
  individual contributions must be provided. The following statements should be 
  used ``X.X. and Y.Y. conceive and designed the experiments; X.X. performed the 
  experiments; X.X. and Y.Y. analyzed the data; W.W. contributed 
  reagents/materials/analysis tools; Y.Y. wrote the paper.'' Authorship must be
  limited to those who have contributed substantially to the work reported.
funding: |
  Please add: ``This research received no external funding'' or ``This research 
  was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded 
  by XXX''. Check carefully that the details given are accurate and use the 
  standard spelling of funding agency names at 
  \url{https://search.crossref.org/funding}, any errors may affect your future 
  funding.
institutionalreview: |
  In this section, you should add the Institutional Review Board Statement and 
  approval number, if relevant to your study. You might choose to exclude 
  this statement if the study did not require ethical approval. Please note 
  that the Editorial Office might ask you for further information. Please add 
  “The study was conducted in accordance with the Declaration of Helsinki, 
  and approved by the Institutional Review Board (or Ethics Committee) of 
  NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies 
  involving humans. OR “The animal study protocol was approved by the 
  Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE 
  (protocol code XXX and date of approval).” for studies involving animals. 
  OR “Ethical review and approval were waived for this study due to REASON 
  (please provide a detailed justification).” OR “Not applicable” for
   studies not involving humans or animals.
informedconsent: |
  Any research article describing a study involving humans should contain this 
  statement. Please add ``Informed consent was obtained from all subjects 
  involved in the study.'' OR ``Patient consent was waived due to REASON 
  (please provide a detailed justification).'' OR ``Not applicable'' for 
  studies not involving humans. You might also choose to exclude this statement 
  if the study did not involve humans.
  
  Written informed consent for publication must be obtained from participating 
  patients who can be identified (including by the patients themselves). Please 
  state ``Written informed consent has been obtained from the patient(s) to 
  publish this paper'' if applicable.
dataavailability: |
  We encourage all authors of articles published in MDPI journals to share 
  their research data. In this section, please provide details regarding where 
  data supporting reported results can be found, including links to publicly 
  archived datasets analyzed or generated during the study. Where no new data 
  were created, or where data is unavailable due to privacy or ethical 
  re-strictions, a statement is still required. Suggested Data Availability 
  Statements are available in section “MDPI Research Data Policies” at 
  \url{https://www.mdpi.com/ethics}.
conflictsofinterest: |
  Declare conflicts of interest or state 'The authors declare no conflict of 
  interest.' Authors must identify and declare any personal circumstances or
  interest that may be perceived as inappropriately influencing the
  representation or interpretation of reported research results. Any role of the
  funding sponsors in the design of the study; in the collection, analyses or 
  interpretation of data in the writing of the manuscript, or in the decision to 
  publish the results must be declared in this section. If there is no role, 
  please state 'The founding sponsors had no role in the design of the study; 
  in the collection, analyses, or interpretation of data; in the writing of the 
  manuscript, an in the decision to publish the results'.
sampleavailability: |
  Samples of the compounds ...... are available from the authors.
supplementary: |
 The following supporting information can be downloaded at:  
 \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.
abbreviations:
  - short: MDPI
    long: Multidisciplinary Digital Publishing Institute
  - short: DOAJ
    long: Directory of open access journals
  - short: TLA
    long: Three letter acronym
  - short: LD 
    long: linear dichroism
bibliography: mybibfile.bib
appendix: appendix.tex
endnotes: false
output: 
  rticles::mdpi_article:
    extra_dependencies: longtable
---


# Article Header Information

The YAML header includes information needed mainly for formatting the front and 
back matter of the article. Required elements include:

```yaml
title: Title of the paper
author:
  - name: first and last name
    affil: |
      One or more comma seperated numbers corresponding to affilitation
      and one or more  comma seperated symbols corresponding 
      optional notes.
    orcid: optional orcid number
affiliation:  
  - num: 1,..., n for each affiliation
    address: required
    email: required
authorcitation: |
  Lastname, F.
correspondence: |
  email@email.com; Tel.: +XX-000-00-0000.
journal: notspecified
type: article
status: submit
```

Journal options are in Table \ref{tab:mdpinames}. The `status` variable should 
generally not be changed by authors. The `type` variable describes 
the type of of submission and defaults to `article` but can be replaced with any of the ones in Table \ref{tab:mdpitype}

```{r mdpitype, echo = FALSE}
type <- c("abstract, addendum, article, book, bookreview, briefreport, 
casereport, comment, commentary, communication, conferenceproceedings, 
correction, conferencereport, entry, expressionofconcern, extendedabstract, 
datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, 
obituary, opinion, projectreport, reply, retraction, review, perspective, 
protocol, shortnote, studyprotocol, systematicreview, supfile, 
technicalnote, viewpoint, guidelines, registeredreport, tutorial")
knitr::kable(rticles::string_to_table(type, 3), 
             col.names = NULL, format = "latex", booktab = TRUE, 
             caption = "MDPI article types.")
```

## Journal Specific YAML variables

```yaml
# for journal Diversity,
# add the Life Science Identifier using:
lsid: http://zoobank.org/urn:lsid:zoobank.org:act:nnnn


# for journal Applied Sciences
# add featured application
featuredapplication: |
  Authors are encouraged to provide a concise 
  description of the specific application or 
  a potential application of the work. This 
  section is not mandatory.

# for the journal Data
# add dataset doi and license
dataset: https://doi.org/10.1000/182
datasetlicense: CC-BY-4.0

# for the journal Toxins
# add key contributions
keycontributions: |
  The breakthroughs or highlights of the manuscript. 
  Authors can write one or two sentences to describe 
  the most important part of the paper.

# for the journal Encyclopedia
encyclopediadef: |
  For entry manuscripts only: please provide a brief overview
  of the entry title instead of an abstract.
entrylink: The Link to this entry published on the encyclopedia platform.

# for the journal Advances in Respiratory Medicine
# add highlights
addhighlights: |
  This is an obligatory section in “Advances in Respiratory Medicine”, 
  whose goal is to increase the discoverability and readability of the
  article via search engines and other scholars. Highlights should not 
  be a copy of the abstract, but a simple text allowing the reader to 
  quickly and simplified find out what the article is about and what can 
  be cited from it. Each of these parts should be devoted up to 2~bullet 
  points.

```


\startlandscape
```{r, mdpinames, echo=FALSE, results='asis'}
mdpi_journals <- "agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, atmosphere, biochem, bioengineering, biologics, biology, biophysica, carbon, climate, clinpract, clockssleep, cmd, covid, crops, dairy, data, ddc, earth, ebj, ecologies, education, environments, environsciproc, epidemiologia, gases, C6H6, CO_HH, NO_HH, N02_HH, NOx_HH, O3_HH, PM10, PM25, S02_HH"

knitr::kable(rticles::string_to_table(mdpi_journals, 8),
             col.names = NULL,
             format = "latex",
             booktabs = TRUE, 
             caption = "MDPI journal names.",
             longtable = TRUE)

```
\finishlandscape

```{r message=FALSE, warning=FALSE, include=FALSE}
# Meted aquí todas las librerias que vamos a usar
packages = c("tidyverse","knitr",'GGally','ggplot2','sf','readr','readxl','dplyr','tidyr','kableExtra','VIM','car')

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE,repos='http://cran.rediris.es')
  }
  library(x, character.only = TRUE)
})

#verify they are loaded
#search()
```



# Introducción

Dentro del marco del Plan Nacional del Instituto Nacional de Estadística (INE) se encuentra el Estudio Estadístico para la Evaluación de la Calidad del Aire, además cuenta con un visor en tiempo real que determina el Índice de Calidad del Aire (ICA) de toda España. 

El ICA clasifica la calidad del aire en 6 categorías: buena, razonablemente buena, regular, desfavorable, muy desfavorable y extremadamente desfavorable. A cada estación se le asigna la categoría más baja en términos de calidad del aire considerando cualquier contaminante que se evalúe. Los contaminantes incluidos en el índice son las Partículas en Suspensión (PM10), Partículas en Suspensión más finas (PM2,5), Ozono Troposférico (O3), Dióxido de Nitrógeno (NO2) y Dióxido de Azufre (SO2).

Todo lo relacionado con el cálculo y límites establecidos del ICA se encuentra legislado por la Resolución del 2 de septiembre de 2020, de la Dirección General de Calidad y Evaluación Ambiental, por la que se modifica el Anexo de la Orden TEC/351/2019, de 18 de marzo, y por la que se aprueba el Índice Nacional de Calidad del Aire. Esta resolución incluye un anexo con la metodología para el cálculo del ICA.



# Procesamiento de Datos

Este trabajo se centra en el procesamiento de datos en el ámbito del análisis de la calidad del aire, abordando un conjunto de datos recopilados durante los años 2018-2022. Inicialmente, el dataset presentaba desafíos inherentes, como la presencia de valores faltantes y una estructura no tidy. La tarea principal consistió en transformar esta información en un formato "tidy" para garantizar la coherencia de los datos.

Con un total de nueve archivos CSV por año que detallan parámetros de mediciones horarias del aire, así como, un archivo de metadatos de cada año con información relevante sobre las estaciones, el dataset depurado establece una base sólida para análisis estadísticos univariantes y bivariantes que nos permitirán abordar el estudio del ICA en los últimos cinco años en la Comunidad Autónoma de Valencia. Por lo que se establece como objetivo en el procesamiento de los datos el compilar la mayor información posible para poder obtener el ICA como variable categórica y sacar conclusiones.


# Formato tidy

En un primer paso, emprendemos la tarea de obtener un conjunto de datos consolidado a partir de nuestros registros. Este proceso implica un análisis detallado de cada año, con el objetivo de integrar las columnas correspondientes a las observaciones diarias de diversos agentes químicos del aire presentes en cada hoja de Excel. En concreto, disponemos de medidas de Partículas en suspensión (PM10 y PM2,5), Ozono troposférico (O3), Dióxido de nitrógeno (NO2), Dióxido de azufre (SO2), Benzeno (C6H6), Monóxido de Carbono (CO), Monóxido de Nitrógeno (NO) y Óxidos de Nitrógeno (NOx), cada una de las variables en un CSV diferente. La finalidad es fusionar estas columnas en una única entidad independiente que contenga todas las observaciones de dicho parámetro durante los cinco años contemplados en el estudio, además de añadirle más variables procedentes de los metadatos de las estaciones.

Este enfoque permite la generación de un conjunto de datos estructurado de manera "tidy", donde cada variable se representa en una columna individual y cada observación de una variable ocupa una fila independiente. La consolidación de las diversas tablas y CSV se lleva a cabo considerando las variables comunes, es decir, los parámetros del aire analizados. Como resultado, nuestro nuevo dataset presenta una primera fila que contiene los nombres representativos de las variables, estableciendo así un marco coherente para el análisis subsiguiente.

Cada archivo se encuentra estructurado originalmente de la siguiente manera:

```{r echo=FALSE, message=FALSE, warning=FALSE}

ds1 <- read_delim("../data/2018/Datos_diarios/NO2_HH_2018.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
kable(head(ds1))                  

```

En la que cada observación corresponde a la medición de las diferentes estaciones de medida de toda España, por lo que inicialmente se filtran los valores a los correspondientes a la CA de Valencia y se restructura para conseguir el valor de la variable en una única columna a lo largo del tiempo.Al ser datos horarios, y para poder establecer diariamente el peor ICA obtenido, se decide computar el valor de la variable como el máximo medido en 24 horas.

Una vez cargados los dataset, se unen entre ellos para obtener un conjunto de datos en el que las variables estan repartidas en cada columna:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
main_root<-'../data'
carpetas<-c('2018','2019','2020','2021','2022')
subcarpeta1<-'Datos_diarios'
z_f<-list()
n=0
for (c in carpetas){
  files<-dir(paste(main_root,c,subcarpeta1,sep='/'))
  df_t<-list()
  names<-list()
  n=n+1
  i=1
  for (file in files){
    ds1 <- read_delim(paste(main_root,c,subcarpeta1,file,sep='/'), 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
    ds1<-ds1[ds1$PROVINCIA==3|ds1$PROVINCIA==12|ds1$PROVINCIA==46,]
    ds_1t <- pivot_longer(ds1, -(PROVINCIA:DIA),names_to = "HORA", values_to = 'Valor', names_prefix = 'H')
    ds_1t <- ds_1t[, -which(names(ds_1t) == "MAGNITUD")]
    ds_1t <- ds_1t[, -which(names(ds_1t) == "PUNTO_MUESTREO")]
    ds_1t<-mutate(ds_1t,Fecha=ymd(paste(ANNO,MES,DIA,sep='-')))
    ds_1t<-select(ds_1t,-c('ANNO','MES','DIA'))
    ds_1t <- ds_1t %>% 
    group_by(Fecha,MUNICIPIO,PROVINCIA,ESTACION) %>%
    summarise(Valor=max(Valor,na.rm=TRUE))
    new_name=paste(strsplit(file,'_')[[1]][1],'num',sep='.')
    ds_1t[new_name]<-ds_1t$Valor
    ds_1t<-select(ds_1t,-c('Valor'))
    df_t[[i]]<-ds_1t
    names[[i]]<-new_name
    i=i+1
  }
  
  z<-merge(df_t[[1]],df_t[[2]],all=TRUE)
  for (i in 3:length(df_t)){
    z<-merge(z,df_t[[i]],all=TRUE)
    
  }
  
z_f[[n]]<-z
}
kable(head(z))

```

Posteriormente se cargan los metadatos correspondientes a cada año y añadimos las variables de Tipo de Área (Urbana, Suburbana y Rural), Latitud y Longitud de la estación de medida, y los nombres del Municipio y Provincia donde se encuentran.


```{r echo=FALSE, message=FALSE, warning=FALSE}
# METADATOS

# Para 2018
meta_df <- read_excel("../data/2018/Metadatos/metainformacion_2018_tcm30-501408.xlsx", sheet="Estaciones evaluación 2018")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[1]]
z_2018<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2018<-z_2018 %>% select(all_of(columnas))
z_2018<-select(z_2018,-c('PROVINCIA','MUNICIPIO'))

# Para 2019
meta_df <- read_excel("../data/2019/Metadatos/metainformacion_2019_tcm30-513561.xlsx", sheet="Estaciones evaluación 2019")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[2]]
z_2019<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2019<-z_2019 %>% select(all_of(columnas))
z_2019<-select(z_2019,-c('PROVINCIA','MUNICIPIO'))

# Para 2020
meta_df <- read_excel("../data/2020/Metadatos/metainformacionestacionesymagnitudes2020_tcm30-531266.xlsx", 
    sheet = "Estaciones")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[3]]
z_2020<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2020<-z_2020 %>% select(all_of(columnas))
z_2020<-select(z_2020,-c('PROVINCIA','MUNICIPIO'))

# Para 2021
meta_df <- read_excel("../data/2021/Metadatos/metainformacion2021_tcm30-545563.xlsx", sheet="Estaciones")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[4]]
z_2021<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2021<-z_2021 %>% select(all_of(columnas))
z_2021<-select(z_2021,-c('PROVINCIA','MUNICIPIO'))

# Para 2022
meta_df <- read_excel("../data/2022/Metadatos/Metainformacion2022.xlsx", sheet="Estaciones")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[5]]
z_2022<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2022<-z_2022 %>% select(all_of(columnas))
z_2022<-select(z_2022,-c('PROVINCIA','MUNICIPIO'))
z_2021$NO.num<-NA
z_2022$NO.num<-NA
df<-rbind(z_2018,z_2019,z_2020,z_2021,z_2022)
kable(head(df))
```

En un primer análisis rápido de las variables observamos una gran cantidad de valores perdidos,

```{r echo=FALSE, message=FALSE, warning=FALSE}

aggr(df[,6:14], prop = FALSE, combined = TRUE, numbers = TRUE, sortVars = TRUE, sortCombs = TRUE)

```

esto es debido a que no en todas las estaciones se miden las mismas variables, para solucionar esto, lo establecido en la metodología es usar el valor de la estación más próxima, por ello y para preparar los datos, se transforma la latitud y longitud a coordenadas proyectadas, de esta manera nos garantizamos poder usar distancias euclideas en caso de imputación por proximidad, además se pasan las fecha a días desde el 01-01-2018, que es la fecha de la primera observación de nuestro conjunto, para establecer una tercera dimensión de proximidad en tiempo. Por último se categoriza la variable de Tipo de Area donde se encuentra la estación.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

df<-mutate(df,TIPO_AREA=as.factor(TIPO_AREA))
origen <- as.Date("2018-01-01")
df$Dias<- as.numeric(difftime(df$Fecha, origen, units = "days"))
df <- st_as_sf(df, coords = c('LONGITUD_G','LATITUD_G'), crs = 4326)
df<-st_transform(df,crs=32630)
X_UTM30N<-c()
Y_UTM30N<-c()
for (i in 1:nrow(df)){
  X_UTM30N<-c(X_UTM30N,st_bbox(df$geometry[i])[[1]])
  Y_UTM30N<-c(Y_UTM30N,st_bbox(df$geometry[i])[[2]])
}
df$X_UTM30N<-X_UTM30N
df$Y_UTM30N<-Y_UTM30N
df<-data.frame(df)
#write.csv(df,'../data/join_datas.csv')
```

# Limpieza de datos

Una vez adquirido un conjunto de datos organizado en formato tidy, es necesario llevar a cabo un proceso de limpieza. Este procedimiento implica la evaluación minuciosa de cada inconsistencia, tales como los valores faltantes (NAs) y los valores atípicos (outliers). Con el propósito de abordar estas discrepancias, se implementan diversos procedimientos y técnicas especializadas.

## Outliers (revisar esto, la regla del 3 sigma es para distribuciones gaussianas, yo la quitaba)

Como es ampliamente conocido, un "outlier" se define como un valor atípico en un conjunto de datos que exhibe una marcada disparidad con respecto al resto de las observaciones. Para analizar la presencia de tales observaciones atípicas, se emplean tres técnicas específicas: la regla de 3 sigma, la regla de Hampel, la regla de boxplot y la regla de los percentiles. Obtuvimos la siguiente tabla:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Método 3-Sigma
df <- read_csv("../data/join_datas.csv") #Hemos guardado el csv con el formato tidy para cargarlo automáticamente y no tener que estas procesando todos los pasos anteriores

find_outliers_3sigma <- function(x) {
  x <- na.omit(x)
  mean_val <- mean(x)
  sd_val <- sd(x)
  lower_bound <- mean_val - 3 * sd_val
  upper_bound <- mean_val + 3 * sd_val
  outliers <- x[x < lower_bound | x > upper_bound]
  positions <- which(x %in% outliers)
  return(list(values = outliers, positions = positions))
}

# Método IQR(Boxplot)

find_outliers_iqr <- function(x) {
  x <- na.omit(x)
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x[x < lower_bound | x > upper_bound]
  positions <- which(x %in% outliers)
  return(list(values = outliers, positions = positions))
}


# Método del Hampel
find_outliers_hampel <- function(df_column, k = 3) {
  x <- na.omit(df_column)
  med <- median(x)
  mad <- median(abs(x - med))
  threshold <- k * 1.4826 * mad
  outliers <- x[abs(x - med) > threshold]
  positions <- which(df_column %in% outliers)
  return(list(values = outliers, positions = positions))
}

# Método de los percentiles
find_outliers_percentile <- function(x) {
  x <- na.omit(x)
  lower_bound <- quantile(x, 0.05)
  upper_bound<- quantile(x, 0.95)
  outliers <- x[x < lower_bound | x > upper_bound]
  positions <- which(x %in% outliers)
  return(list(values = outliers, positions = positions))
}
```


```{r echo=FALSE, message=FALSE, warning=FALSE}

# Aplicamos a nuestros datos los métodos anteriores

columnas_analizar <- c("NO2.num", "NOx.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")

outliers_3sigma_list <- lapply(df[columnas_analizar], find_outliers_3sigma)
outliers_hampel_list <- lapply(df[columnas_analizar], find_outliers_hampel)
outliers_iqr_list <- lapply(df[columnas_analizar], find_outliers_iqr)
outliers_percentile_list <- lapply(df[columnas_analizar], find_outliers_percentile)

#print("Outliers detectados usando la regla 3-sigma:")
#print(outliers_3sigma_list)

#print("Outliers detected using Hampel identifier:")
#print(outliers_hampel_list)

#print("Outliers detected using IQR(Boxplot):")
#print(outliers_iqr_list)

#print("Outliers detected using IQR:")
#print(outliers_percentile_list)

```


Aquí utilizamos los códigos implementados anteriormente en cada una de las variables numéricas para detectar los outliers

```{r echo=FALSE, message=FALSE, warning=FALSE}

outliers_summary <- data.frame(Chemical_Element = rep(columnas_analizar, each = 4),
                                Method = rep(c("3-Sigma", "IQR", "Hampel", "Percentil"), times = length(columnas_analizar)),
                                Outliers_Count = c(
                                  sapply(outliers_3sigma_list, function(x) length(x$values)),
                                  sapply(outliers_iqr_list, function(x) length(x$values)),
                                  sapply(outliers_hampel_list, function(x) length(x$values)),
                                  sapply(outliers_percentile_list, function(x) length(x$values))
                                ))
```



```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

outliers_summary_transformed <- outliers_summary %>%
  pivot_wider(names_from = Method, values_from = Outliers_Count)

print(outliers_summary_transformed)
```

En esta tabla visualizamos un recuento de la cantidad de outliers detectados con cada método. 


Ahora bien, tenemos que analizar que consideramos un valor atípico en nuestro caso. Para ello, debemos establecer algún criterio. Después de documentarnos, encontramos en el BOE (Boletín oficial del Estado) la siguiente tabla:


| $S0_{2}$ | PM2,5 | PM10 | $O_{3}$ | $NO_{2}$ | CATEGORÍA DEL ÍNDICE |
|--------|-------|------|-------|--------|----------------------|
| 751-1250| 76-800| 151-1200| 381-800 | 341-1000 | Extremadamente Desfavorable|


Para utilizar la detección de valores atípicos y asignarles esta categoría, en nuestro caso calculamos tanto el valor máximo como el mínimo para cada uno de los parámetros químicos. Este proceso nos permite evaluar si dichos valores caen fuera del rango establecido en la tabla anterior, lo que justificaría su clasificación como outliers. Tras realizar estos cálculos, hemos verificado que los valores identificados como outliers son válidos y, por ende, hemos decidido mantenerlos en el conjunto de datos.


```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
(max_values <- lapply(outliers_3sigma_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_3sigma_list, function(x) min(x$values, na.rm = TRUE)))

(max_values <- lapply(outliers_hampel_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_hampel_list, function(x) min(x$values, na.rm = TRUE)))

(max_values <- lapply(outliers_iqr_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_iqr_list, function(x) min(x$values, na.rm = TRUE)))

(max_values <- lapply(outliers_percentile_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_percentile_list, function(x) min(x$values, na.rm = TRUE)))
```

## Imputación NAs

El siguiente paso en el proceso de limpieza de nuestro conjunto de datos implica la imputación de los valores faltantes, comúnmente conocidos como NAs. Para abordar esta tarea, hemos estudiado diversas técnicas en clase, entre las cuales se incluyen: la eliminación de datos, la imputación con un valor estimado basado en los valores conocidos, la imputación mediante un modelo predictivo o la imputación al valor más cercano utilizando el algoritmo KNN (k-vecinos más cercanos).

Para ello se realiza un análisis preliminar de los datos, calculando las correlaciones entre variables.

A pesar de que el ICA solo usa 5 de las 9 variables que disponemmos, se ha decidido dejarlas inicialmente por si exsite alguna relación que las conviertan en posibles predictores de datos faltantes, por lo que analizamos las covarianzas y correlaciones entre ellos,

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggcorr(df[,6:14])
kable(cov(na.omit(df[,6:14])), title='Covarianzas')
```

observandose una fuerte correlación entre los Óxidos Nitroso.

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggpairs(df[,8:10])
```


y decidiendose usar las variables NO y NOx para inferir los datos faltantes del NO2, compuesto necesario para calcular el ICA. 

Tras analizar los datos de NOx y NO se observa que no existen datos en los años 2021 y 2022 por lo que se acota la posibilidad de inferencia del 2018 al 2020.

De manera que se entrena un modelo lineal múltiple con los siguientes resultados para el conjunto de test:

```{r echo=FALSE, message=FALSE, warning=FALSE}
df_imp<-df[df$Fecha>='2018-01-01' & df$Fecha<='2020-12-31',]
sample <- sample(c(TRUE, FALSE), nrow(df_imp), replace=TRUE, prob=c(0.8,0.2))
train  <- df_imp[sample, ]
test   <- df_imp[!sample, ]
lm.NO2<-lm(NO2.num~NOx.num+NO.num,train)
print(summary(lm.NO2))
predict<-predict.lm(lm.NO2,test)
RMSE<-sqrt(mean((test$NO2.num-predict) ^ 2, na.rm=T))
cat('RMSE con el conjunto de test= ',RMSE)
plot(lm.NO2)
```

El problema es que la mayoría de estaciones que no obtienen los datos de NO2 tampoco los obtiene de la familia de los Óxidos Nitrosos.

Es por ello que se decide imputar por proximidad geográfica y en el tiempo los datos de los Óxidos Nitrosos para posteriormente usarlo para inferir los datos del NO2 con el modelo entrenado.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
df_imp2<-kNN(df_imp,'NO.num',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Dias'))
df_imp2<-kNN(df_imp2,'NOx.num',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Dias'))
predict<-predict.lm(lm.NO2,test)
RMSE<-sqrt(mean((test$NO2.num-round(predict)) ^ 2, na.rm=T))
df_imp2[is.na(df_imp2$NO2.num),]$NO2.num<-predict.lm(lm.NO2,df_imp2[is.na(df_imp2$NO2.num),])
df_imp2<-mutate(df_imp2,NO2.num=round(df_imp2$NO2.num))
df_imp2[df_imp2$NO2.num<0,]$NO2.num<-NA
df[df$Fecha>='2018-01-01' & df$Fecha<='2020-12-31',]$NO2.num<-df_imp2$NO2.num

```

Para finalizar se imputan el resto de datos faltantes por proximidad geográfica y temporal y se eliminan las variables que no necesitamos para el cálculo del ICA.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
df<-df %>% select(-c('C6H6.num','CO.num','NO.num','NOx.num'))
df_imp1<-kNN(df,'SO2.num',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Dias'))
df_imp1<-kNN(df_imp1,'O3.num',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Dias'))
df_imp1<-kNN(df_imp1,'PM25.num',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Dias'))
df_imp1<-kNN(df_imp1,'PM10.num',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Dias'))
df_imp1<-kNN(df_imp1,'NO2',k=1, dist_var=c('X_UTM30N','Y_UTM30N','Fecha.num'))
no_valids_var<-c('NO2_imp','O3_imp','PM10_imp','PM25_imp','SO2_imp')
df2<-df %>% select(all_of(no_valids_var))
df<-df %>% select(-no_valids_var)
```


## Calculo ICA.Aquí se debe meter el código para calcular el ICA de cada variable



 
 
Una vez se obtiene el valor categórico de cada variable, se establece el Indice global diario como el peor de los individuales.

```{r eval=FALSE, message=TRUE, warning=TRUE, include=FALSE}
df<- read_csv("../data/imputed_cat.csv", 
    col_types = cols(Fecha = col_date(format = "%Y-%m-%d")))
df<-df[,-1]
df2<-df %>% select(c(ends_with('.cat')))
ica<-data.frame(ICA=NA)
for( i in 1:nrow(df2)){
  a<-arrange(pivot_longer(df2[i,],cols=colnames(df2), names_to='A',values_to = 'ICA'),ICA)
  ica<-rbind(ica,a[5,'ICA'])
}
df$ICA<-na.omit(ica$ICA)
```


```{r message=FALSE, warning=FALSE}
df <- read_csv("../data/dataset_final.csv", 
    col_types = cols(...1 = col_skip(), ...2 = col_skip(), 
        Fecha = col_date(format = "%Y-%m-%d")))
levels<-c('Buena','Razonablemente Buena','Regular','Desfavorable','Muy desfavorable','Extremadamente desfavorable')
df<-df%>% mutate(TIPO_AREA=as.factor(TIPO_AREA))%>%mutate(across(c(ends_with('.cat')), factor,level=levels))%>% mutate(ICA=factor(ICA,level=levels))
kable(head(df))
```



# Análisis Univariante


En esta sección, procederemos a llevar a cabo un análisis univariante sobre el conjunto de datos previamente procesado. El propósito principal de esta evaluación consiste en examinar individualmente cada variable presente en el dataset, con el objetivo de obtener un panorama detallado de su distribución y variabilidad.

En primer lugar, para tener una visión global de este dataset sacamos un resumen estadístico


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Resumen estadístico
data <- df
summary(data)

```


Una vez obtenidos los estadísticos. Visualizamos nuestras variables.


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(grid)

create_histogram <- function(data, contaminante, x_label) {
  ggplot(data, aes(x = get(paste0(contaminante, ".num")))) +
    geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribución de", contaminante), x = x_label) +
    theme_minimal()
}

graficas <- lapply(c("NO2", "O3", "PM10", "PM25", "SO2"), function(contaminante) {
  create_histogram(data, contaminante, paste("Concentración de", contaminante))
})

# Combinamos las gráficas en una única representación
multiplot <- function(..., plotlist = NULL, cols = 1, layout = NULL) {
  require(grid)

  
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  
  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }

  if (numPlots == 1) {
    print(plots[[1]])

  } else {
    
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    for (i in 1:numPlots) {
      
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

multiplot(plotlist = graficas, cols = 2)

```

La representación gráfica previa proporciona una visión general de la concentración de cada elemento en nuestro conjunto de datos. La representación más notable en términos de su estructura y forma corresponde al elemento $O_{3}$, ya que parece seguir una distribución gaussiana. Los datos están distribuidos de manera uniforme a lo largo del eje x, que representa la concentración, y muestran una distribución alrededor de la media y la mediana, ambas con valores muy similares, como se confirmó en los estadísticos previamente descritos. (88.60 y 88.00 respectivamente).

En contraste, las representaciones de los otros elementos exhiben una asimetría significativa, con valores relativamente altos al principio seguidos de una disminución. Estas observaciones son respaldadas numéricamente por los estadísticos calculados anteriormente. Por ejemplo, al considerar el elemento $S0_{2}$, se observa un valor máximo de 239,000. Sin embargo, la media calculada es de 6,359, un valor considerablemente distante del máximo mencionado. Además, el valor mínimo es 1, mientras que la mediana es 4,000. Estas discrepancias justifican la representación gráfica de esta información. Esto mismo sucedería con los elementos restantes.


A continuación, procederemos a visualizar la evolución de los compuestos químicos a lo largo del tiempo, con el objetivo de realizar un análisis más exhaustivo sobre la calidad del aire.


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

# Crear gráficos de densidad
ggplot(data, aes(x = NO2.num, fill = "NO2")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de NO2", x = "Concentración de NO2")

ggplot(data, aes(x = O3.num, fill = "O3")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de O3", x = "Concentración de O3")

ggplot(data, aes(x = PM10.num, fill = "PM10")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de PM10", x = "Concentración de PM10")

ggplot(data, aes(x = PM25.num, fill = "PM2.5")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de PM2.5", x = "Concentración de PM2.5")

ggplot(data, aes(x = SO2.num, fill = "SO2")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de SO2", x = "Concentración de SO2")

ggplot(data, aes(x = Dias)) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de Días", x = "Número de Días")

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  group_by(year = lubridate::year(Fecha)) %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de líneas superpuestas con datos escalados y facet_wrap por año
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled, color = "NO2")) +
  geom_line(aes(y = O3_scaled, color = "O3")) +
  geom_line(aes(y = PM10_scaled, color = "PM10")) +
  geom_line(aes(y = PM25_scaled, color = "PM25")) +
  geom_line(aes(y = SO2_scaled, color = "SO2")) +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Años",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal() +
  facet_wrap(~year, scales = "free_y", ncol = 4)



```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

# Convertir Fecha a formato de fecha
data$Fecha <- as.Date(data$Fecha)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  group_by(year = lubridate::year(Fecha)) %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de líneas superpuestas con datos escalados, facet_wrap por año y suavizado por media mensual
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled, color = "NO2")) +
  geom_line(aes(y = O3_scaled, color = "O3")) +
  geom_line(aes(y = PM10_scaled, color = "PM10")) +
  geom_line(aes(y = PM25_scaled, color = "PM25")) +
  geom_line(aes(y = SO2_scaled, color = "SO2")) +
  geom_smooth(aes(y = NO2_scaled), method = "loess", se = FALSE, color = "red") +
  geom_smooth(aes(y = O3_scaled), method = "loess", se = FALSE, color = "blue") +
  geom_smooth(aes(y = PM10_scaled), method = "loess", se = FALSE, color = "green") +
  geom_smooth(aes(y = PM25_scaled), method = "loess", se = FALSE, color = "purple") +
  geom_smooth(aes(y = SO2_scaled), method = "loess", se = FALSE, color = "orange") +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Años",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal() +
  facet_wrap(~year, scales = "free_y", ncol = 4)

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  group_by(year = lubridate::year(Fecha)) %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de puntos superpuestos con datos escalados y facet_wrap por año
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled, color = "NO2"), size = 2, alpha = 0.7) +
  geom_line(aes(y = O3_scaled, color = "O3"), size = 2, alpha = 0.7) +
  geom_line(aes(y = PM10_scaled, color = "PM10"), size = 2, alpha = 0.7) +
  geom_line(aes(y = PM25_scaled, color = "PM25"), size = 2, alpha = 0.7) +
  geom_line(aes(y = SO2_scaled, color = "SO2"), size = 2, alpha = 0.7) +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Años",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal() +
  facet_wrap(~year, scales = "free_y", ncol = 4)

```



```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)

# Convertir Fecha a formato de fecha
data$Fecha <- as.Date(data$Fecha)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de líneas superpuestas con datos escalados y suavizado por media mensual
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = O3_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = PM10_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = PM25_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = SO2_scaled), alpha = 0.5, size = 1) +
  geom_smooth(aes(y = NO2_scaled), method = "loess", se = FALSE, color = "red") +
  geom_smooth(aes(y = O3_scaled), method = "loess", se = FALSE, color = "blue") +
  geom_smooth(aes(y = PM10_scaled), method = "loess", se = FALSE, color = "green") +
  geom_smooth(aes(y = PM25_scaled), method = "loess", se = FALSE, color = "purple") +
  geom_smooth(aes(y = SO2_scaled), method = "loess", se = FALSE, color = "orange") +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Fecha",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal()


```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

# Cargar tus datos
# Por ejemplo, asumiendo que tus datos se llaman "data"
# data <- read.csv("tu_archivo.csv")

# Crear un gráfico de densidad superpuestas
ggplot(data, aes(x = NO2.num, fill = "NO2")) +
  geom_density(alpha = 0.5) +
  geom_density(aes(x = O3.num, fill = "O3"), alpha = 0.5) +
  geom_density(aes(x = PM10.num, fill = "PM10"), alpha = 0.5) +
  geom_density(aes(x = PM25.num, fill = "PM25"), alpha = 0.5) +
  geom_density(aes(x = SO2.num, fill = "SO2"), alpha = 0.5) +
  labs(title = "Densidad de Compuestos Químicos",
       x = "Concentración",
       y = "Densidad") +
  scale_fill_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal()

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Tendencia temporal de NO2.num
ggplot(data, aes(x =Fecha, y = NO2.num)) +
  geom_line() +
  labs(title = "Tendencia Temporal de NO2.num", x = "Fecha", y = "Valor")

```






Pensar que hacer con los nombres

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Cargar biblioteca ggplot2
library(ggplot2)

# Crear un gráfico de barras apiladas con etiquetas del eje x rotadas
ggplot(data, aes(x = ICA, fill = N_PROVINCIA)) +
  geom_bar(position = "stack") +
  labs(title = "Frecuencias de Categorías", x = "Tipo de Área") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```



```{r echo=FALSE, message=FALSE, warning=FALSE}
# Cargar bibliotecas necesarias
library(ggplot2)

# Crear una columna 'variable' con los nombres de las variables
data$variable <- rep(c("NO2", "O3", "PM10", "PM25", "SO2"), length.out = nrow(data))

# Crear un gráfico de barras con facet_wrap
ggplot(data, aes(x = N_PROVINCIA, fill = variable)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(title = "Relación entre Elementos Químicos y Provincias", x = "Provincia", y = "Frecuencia") +
  scale_fill_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  facet_wrap(~variable, scales = "free_y", ncol = 1) +
  theme_minimal()


```


```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(zoo)
data$Fecha <- as.Date(data$Fecha)
(data.zoo <- zoo(data[, -c(1:3, 13:15)], order.by = data$Fecha))

```













# Análisis Bivariante

Para realizar el análisis bivariante primer observaremos como se relacionan todas las variables entre sí

Primero vamos a realizar un `ggpairs` y un `ggcorr`para relacionar todos los elementos químicos con 
todos (valores numéricos)

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(GGally)
num_cols <- c("NO2.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")
ggpairs(df[num_cols])
```

 

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggcorr(df[num_cols])
```
Como podemos ver, la relación de las variables no es clara. De forma general podemos decir que las variables
son bastante independientes entre sí.Como excepción podríamos hablar de las variables PM10 y PM25


Ahora mostraremos las matrices de covaraianza y correlación con Pearson y Spearman
```{r echo=FALSE, message=FALSE, warning=FALSE}
Cov_Mat <- cov(df[num_cols])

print("Matriz de covarianza")
print(Cov_Mat)

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
Cor_Mat_S <- cor(df[num_cols], method = "spearman")
Cor_Mat_P <- cor(df[num_cols], method = "pearson")

print("Matriz de correlacion (Spearman)")
print(Cor_Mat_S)
print("Matriz de correlacion (Pearson)")
print(Cor_Mat_P)

```

Ya que hemos visto que las variables PM10 y PM25 parece que tengan alguna relación, vamos a relacionar sus equivalentes categóricas para ver si la hipótesis nula de que son independientes se cumple o no.


```{r message=FALSE, warning=FALSE}

chisq.test(df$PM10.cat, df$PM25.cat, correct = F)

```

Podemos ver que el p-valor es muy pequeño, luego podemos rechazar la hipótesis de que no están relacionadas.


Este efecto es más grande en las categóricas, ya que "medimos" por intervalos. Por lo que podemos decir que sí tienen alguna relación entre ellas


## NO2

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Gráfico de dispersión entre NO2.num y O3.num
ggplot(df, aes(x = NO2.num, y = O3.num)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot entre NO2.num y O3.num", x = "NO2.num", y = "O3.num")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Matriz de correlación
cor(df[, c("NO2.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")])

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Gráfico de barras agrupadas para la variable TIPO_AREA y NO2.num
ggplot(df, aes(x = TIPO_AREA, y = NO2.num, fill = TIPO_AREA)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(title = "Promedio de NO2.num por TIPO_AREA", x = "TIPO_AREA", y = "Promedio NO2.num")

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Mapa de calor de la matriz de correlación
cor_matrix <- cor(df[, c("NO2.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")])
heatmap(cor_matrix, col = colorRampPalette(c("blue", "white", "red"))(50), 
        main = "Mapa de Calor de Correlación")

```



```{r echo=FALSE, message=FALSE, warning=FALSE}
# Gráfico de líneas para NO2.num y O3.num a lo largo del tiempo
ggplot(df, aes(x = Fecha)) +
  geom_line(aes(y = NO2.num, color = "NO2")) +
  geom_line(aes(y = O3.num, color = "O3")) +
  labs(title = "Evolución de NO2.num y O3.num a lo largo del tiempo", x = "Fecha", y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue"))

```



```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Boxplot para comparar la distribución de NO2.num en diferentes estaciones
ggplot(df, aes(x = year, y = NO2.num, fill = ESTACION)) +
  geom_boxplot() +
  labs(title = "Distribución de NO2.num por Estación", x = "Estación", y = "NO2.num")

```



```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Cargar datos geoespaciales de países (usaremos 'world' en lugar de 'ne_countries')
world <- ne_download(scale = 110, returnclass = "sf")

# Crear un objeto sf con las coordenadas de tu conjunto de datos
data_sf <- st_as_sf(data, coords = c("X_UTM30N", "Y_UTM30N"), crs = st_crs(world))

# Crear el mapa
ggplot() +
  geom_sf(data = world) +
  geom_sf(data = data_sf, color = "red", size = 2) +
  labs(title = "Ubicación de las coordenadas en España") +
  theme_minimal()
```

Vamos a mostrar ahora un histograma de la cantidad de días en los cuales ha habido cada uno de los diferentes
índices de la calidad del aire. Para calcular esto debemos coger todas las mediciones de un mismo día y quedarnos 
con la peor, y esta es la calidad de aire para ese día.

```{r echo=FALSE, message=TRUE, warning=TRUE}


ggplot(df, aes(x = ICA, fill = ICA)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "Histograma de la calidad de aire diaria (peor parámetro diario)", x = "ICA", y = "Días") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

Vamos ahora a probar, por ejemplo, a sacar la misma gráfica usando solo los datos de la provincia de València

```{r echo=FALSE, message=TRUE, warning=TRUE}

df_valencia <- subset(df, N_PROVINCIA == "VALENCIA")

ggplot(df_valencia, aes(x = ICA, fill = factor(ICA))) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "Histograma de la calidad de aire diaria (peor parámetro diario)", x = "ICA", y = "Días") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



Podemos observar que la distribución de los datos parece mantenerse en ambos casos. Hay pocos días donde la calidad
del aire sea buena. Por otro lado el parámetro más frecuente es el de razonablemente buena, a partir del cual
los sucesivos van bajando hasta llegar a extremadamente desfavorable.

```{r echo=FALSE, message=TRUE, warning=TRUE}
mosaicplot(table(df$ICA, df$TIPO_AREA), main = "Mosaic Plot of ICA and TIPO_AREA", las = 3, color = c("red", "green", "blue"))

```
Con el mosaic plot podemos observar como a pesar de que la cantidad de días que el aire tiene una calidad buena no es muy diferente entre las zonas, en las zonas rurales hay menos días con calidades peores.








```{r}
# # Instalar y cargar las librerías
# library(sf)
# library(leaflet)
# 
# # Cargar datos desde el archivo JSON
# ruta_al_archivo <- "../data/CV-MAP.json"
# datos_sf <- st_read(ruta_al_archivo)
# 
# # Crear un mapa interactivo con Leaflet
# mapa <- leaflet() %>%
#   addTiles() %>%
#   addMarkers(data = datos_sf, ~X_UTM30N, ~Y_UTM30N, popup = ~as.character(geometry))
# 
# # Mostrar el mapa
# mapa

```

# Conclusiones











































# Results

This section may be divided by subheadings. It should provide a concise and
precise description of the experimental results, their interpretation as well
as the experimental conclusions that can be drawn.

## Subsection Heading Here

Subsection text here.

### Subsubsection Heading Here

Bulleted lists look like this:

* First bullet
* Second bullet
* Third bullet

Numbered lists can be added as follows:

1. First item
2. Second item
3. Third item

The text continues here.

## Figures, Tables and Schemes

All figures and tables should be cited in the main text as Figure \ref{fig:fig1}, 
\ref{tab:tab1}, etc. To get cross-reference to figure generated by R chunks 
include the `\\label{}` tag in the `fig.cap` attribute of the R chunk: 
`fig.cap = "Fancy Caption\\label{fig:plot}"`.

```{r fig1, echo=FALSE, fig.width=3, fig.cap="A figure added with a code chunk.\\label{fig:fig1}"}
x = rnorm(10)
y = rnorm(10)
plot(x, y)
```


When making tables using `kable`, it is suggested to use
the `format="latex"` and `tabl.envir="table"` arguments
to ensure table numbering and compatibility with the mdpi
document class.

```{r tab1, echo=FALSE}
knitr::kable(mtcars[1:5, 1:3], format = "latex", 
             booktabs = TRUE, 
             caption = "This is a table caption. Tables should be placed in the 
             main text near to the first time they are~cited.", 
             align = 'ccc', centering = FALSE,
             table.envir = "table", position = "H")
```



For a very wide table, landscape layouts are allowed.


\startlandscape

```{r tab2, echo=FALSE}
df <- data.frame(`Title 1` = c("Entry 1", "Entry 2"),
                 `Title 2` = c("Data", "Data"),
                 `Title 3` = c("Data", "Data"),
                 `Title 4` = c("This cell has some longer content that runs over
                               two lines",
                               "Data"))
knitr::kable(df, format = "latex", 
             booktabs = TRUE, 
             caption = "This is a very wide table", 
             align = 'ccc', centering = FALSE,
             table.envir = "table", position = "H")
```

\finishlandscape

## Formatting of Mathematical Components

This is an example of an equation:

$$
a = 1.
$$

If you want numbered equations use Latex and wrap in the equation environment:

\begin{equation}
a = 1,
\end{equation}

the text following an equation need not be a new paragraph. Please punctuate 
equations as regular text.

```{comment}
If the documentclass option "submit" is chosen, please insert a blank line before and after any math environment (equation and eqnarray environments). This ensures correct linenumbering. The blank line should be removed when the documentclass option is changed to "accept" because the text following an equation should not be a new paragraph.
```

This is the example 2 of equation:

\begin{adjustwidth}{-\extralength}{0cm}
\begin{equation}
a = b + c + d + e + f + g + h + i + j + k + l + m + n + o + p + q + r + s + t + 
u + v + w + x + y + z
\end{equation}
\end{adjustwidth}

Theorem-type environments (including propositions, lemmas, corollaries etc.) 
can be formatted as follows:

Example of a theorem:

::: {.Theorem latex=true}
Example text of a theorem
:::

The text continues here. Proofs must be formatted as follows:

Example of a proof:

::: {.proof latex="[Proof of Theorem1]"}
Text of the proof. Note that the phrase ``of Theorem 1'' is optional if it is 
clear which theorem is being referred to.
:::

The text continues here.

# Discussion

Authors should discuss the results and how they can be interpreted in 
perspective of previous studies and of the working hypotheses. The findings and 
their implications should be discussed in the broadest context possible. Future 
research directions may also be highlighted.

# Conclusion

This section is not mandatory, but can be added to the manuscript if the
discussion is unusually long or complex.

# Patents

This section is not mandatory, but may be added if there are patents resulting
from the work reported in this manuscript.
