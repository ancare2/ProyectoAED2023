---
title: Trabajo Análisis Exploratorio de Datos
author:
  - name: Luis Miguel Rioja Gallo, Anna Cabrerizo Requena
    affil: 1,2,\ddagger,*
    orcid: 0000-0003-3293-2315
  - name: John Doe
    affil: 2, \dagger, \ddagger
affiliation:
  - num: 1
    address: |
      Universitat de València - 
      Etse. Burjassot, València
    email: ancare2@alumne.uv.es
  - num: 2
    address: |
      Your department
      Street, City, Country
    email: mail@mail.com
# author citation list in chicago format
authorcitation: |
  Leutnant, D.; Doe, J.
# firstnote to eighthnote
firstnote: |
  Current address: Updated affiliation
secondnote: |
  These authors contributed equally to this work.
correspondence: |
  leutnant@fh-muenster.de; Tel.: +XX-000-00-0000.
# document options
journal: notspecified
type: article
status: submit
# front matter
simplesummary: |
  A Simple summary goes here.
abstract: |
  A single paragraph of about 200 words maximum. For research articles, 
  abstracts should give a pertinent overview of the work. We strongly encourage
  authors to use the following style of structured abstracts, but without 
  headings: 1) Background: Place the question addressed in a broad context and
  highlight the purpose of the study; 2) Methods: Describe briefly the main
  methods or treatments applied; 3) Results: Summarize the article's main 
  findings; and 4) Conclusion: Indicate the main conclusions or interpretations. 
  The abstract should be an objective representation of the article, it must not 
  contain results which are not presented and substantiated in the main text and 
  should not exaggerate the main conclusions.
# back matter
keywords: |
  keyword 1; keyword 2; keyword 3 (list three to ten pertinent keywords specific 
  to the article, yet reasonably common within the subject discipline.).
acknowledgement: |
  All sources of funding of the study should be disclosed. Please clearly 
  indicate grants that you have received in support of your research work. 
  Clearly state if you received funds for covering the costs to publish in open 
  access.
authorcontributions: |
  For research articles with several authors, a short paragraph specifying their 
  individual contributions must be provided. The following statements should be 
  used ``X.X. and Y.Y. conceive and designed the experiments; X.X. performed the 
  experiments; X.X. and Y.Y. analyzed the data; W.W. contributed 
  reagents/materials/analysis tools; Y.Y. wrote the paper.'' Authorship must be
  limited to those who have contributed substantially to the work reported.
funding: |
  Please add: ``This research received no external funding'' or ``This research 
  was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded 
  by XXX''. Check carefully that the details given are accurate and use the 
  standard spelling of funding agency names at 
  \url{https://search.crossref.org/funding}, any errors may affect your future 
  funding.
institutionalreview: |
  In this section, you should add the Institutional Review Board Statement and 
  approval number, if relevant to your study. You might choose to exclude 
  this statement if the study did not require ethical approval. Please note 
  that the Editorial Office might ask you for further information. Please add 
  “The study was conducted in accordance with the Declaration of Helsinki, 
  and approved by the Institutional Review Board (or Ethics Committee) of 
  NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies 
  involving humans. OR “The animal study protocol was approved by the 
  Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE 
  (protocol code XXX and date of approval).” for studies involving animals. 
  OR “Ethical review and approval were waived for this study due to REASON 
  (please provide a detailed justification).” OR “Not applicable” for
   studies not involving humans or animals.
informedconsent: |
  Any research article describing a study involving humans should contain this 
  statement. Please add ``Informed consent was obtained from all subjects 
  involved in the study.'' OR ``Patient consent was waived due to REASON 
  (please provide a detailed justification).'' OR ``Not applicable'' for 
  studies not involving humans. You might also choose to exclude this statement 
  if the study did not involve humans.
  
  Written informed consent for publication must be obtained from participating 
  patients who can be identified (including by the patients themselves). Please 
  state ``Written informed consent has been obtained from the patient(s) to 
  publish this paper'' if applicable.
dataavailability: |
  We encourage all authors of articles published in MDPI journals to share 
  their research data. In this section, please provide details regarding where 
  data supporting reported results can be found, including links to publicly 
  archived datasets analyzed or generated during the study. Where no new data 
  were created, or where data is unavailable due to privacy or ethical 
  re-strictions, a statement is still required. Suggested Data Availability 
  Statements are available in section “MDPI Research Data Policies” at 
  \url{https://www.mdpi.com/ethics}.
conflictsofinterest: |
  Declare conflicts of interest or state 'The authors declare no conflict of 
  interest.' Authors must identify and declare any personal circumstances or
  interest that may be perceived as inappropriately influencing the
  representation or interpretation of reported research results. Any role of the
  funding sponsors in the design of the study; in the collection, analyses or 
  interpretation of data in the writing of the manuscript, or in the decision to 
  publish the results must be declared in this section. If there is no role, 
  please state 'The founding sponsors had no role in the design of the study; 
  in the collection, analyses, or interpretation of data; in the writing of the 
  manuscript, an in the decision to publish the results'.
sampleavailability: |
  Samples of the compounds ...... are available from the authors.
supplementary: |
 The following supporting information can be downloaded at:  
 \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.
abbreviations:
  - short: MDPI
    long: Multidisciplinary Digital Publishing Institute
  - short: DOAJ
    long: Directory of open access journals
  - short: TLA
    long: Three letter acronym
  - short: LD 
    long: linear dichroism
bibliography: mybibfile.bib
appendix: appendix.tex
endnotes: false
output: 
  rticles::mdpi_article:
    extra_dependencies: longtable
---


# Article Header Information

The YAML header includes information needed mainly for formatting the front and 
back matter of the article. Required elements include:

```yaml
title: Title of the paper
author:
  - name: first and last name
    affil: |
      One or more comma seperated numbers corresponding to affilitation
      and one or more  comma seperated symbols corresponding 
      optional notes.
    orcid: optional orcid number
affiliation:  
  - num: 1,..., n for each affiliation
    address: required
    email: required
authorcitation: |
  Lastname, F.
correspondence: |
  email@email.com; Tel.: +XX-000-00-0000.
journal: notspecified
type: article
status: submit
```

Journal options are in Table \ref{tab:mdpinames}. The `status` variable should 
generally not be changed by authors. The `type` variable describes 
the type of of submission and defaults to `article` but can be replaced with any of the ones in Table \ref{tab:mdpitype}

```{r mdpitype, echo = FALSE}
type <- c("abstract, addendum, article, book, bookreview, briefreport, 
casereport, comment, commentary, communication, conferenceproceedings, 
correction, conferencereport, entry, expressionofconcern, extendedabstract, 
datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, 
obituary, opinion, projectreport, reply, retraction, review, perspective, 
protocol, shortnote, studyprotocol, systematicreview, supfile, 
technicalnote, viewpoint, guidelines, registeredreport, tutorial")
knitr::kable(rticles::string_to_table(type, 3), 
             col.names = NULL, format = "latex", booktab = TRUE, 
             caption = "MDPI article types.")
```

## Journal Specific YAML variables

```yaml
# for journal Diversity,
# add the Life Science Identifier using:
lsid: http://zoobank.org/urn:lsid:zoobank.org:act:nnnn


# for journal Applied Sciences
# add featured application
featuredapplication: |
  Authors are encouraged to provide a concise 
  description of the specific application or 
  a potential application of the work. This 
  section is not mandatory.

# for the journal Data
# add dataset doi and license
dataset: https://doi.org/10.1000/182
datasetlicense: CC-BY-4.0

# for the journal Toxins
# add key contributions
keycontributions: |
  The breakthroughs or highlights of the manuscript. 
  Authors can write one or two sentences to describe 
  the most important part of the paper.

# for the journal Encyclopedia
encyclopediadef: |
  For entry manuscripts only: please provide a brief overview
  of the entry title instead of an abstract.
entrylink: The Link to this entry published on the encyclopedia platform.

# for the journal Advances in Respiratory Medicine
# add highlights
addhighlights: |
  This is an obligatory section in “Advances in Respiratory Medicine”, 
  whose goal is to increase the discoverability and readability of the
  article via search engines and other scholars. Highlights should not 
  be a copy of the abstract, but a simple text allowing the reader to 
  quickly and simplified find out what the article is about and what can 
  be cited from it. Each of these parts should be devoted up to 2~bullet 
  points.

```


\startlandscape
```{r, mdpinames, echo=FALSE, results='asis'}
mdpi_journals <- "agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, atmosphere, biochem, bioengineering, biologics, biology, biophysica, carbon, climate, clinpract, clockssleep, cmd, covid, crops, dairy, data, ddc, earth, ebj, ecologies, education, environments, environsciproc, epidemiologia, gases, C6H6, CO_HH, NO_HH, N02_HH, NOx_HH, O3_HH, PM10, PM25, S02_HH"

knitr::kable(rticles::string_to_table(mdpi_journals, 8),
             col.names = NULL,
             format = "latex",
             booktabs = TRUE, 
             caption = "MDPI journal names.",
             longtable = TRUE)

```
\finishlandscape




# Introducción


# Procesamiento de Datos

Este trabajo se centra en el procesamiento de datos en el ámbito del análisis de la calidad del aire, abordando un conjunto de datos recopilados durante los años 2019-2022. Inicialmente, el dataset presentaba desafíos inherentes, como la presencia de valores faltantes y una estructura no tidy. La tarea principal consistió en transformar esta información en un formato "tidy", eliminamos valores ausentes y garantizamos la coherencia de los datos.

Con un total de nueve archivos Excel que detallan parámetros de mediciones horarias del aire, el dataset depurado establece una base sólida para análisis estadísticos univariantes y bivariantes que nos permitirán abordar las cuestiones principales de nuestro estudio.


# Formato tidy

En un primer paso, emprendemos la tarea de obtener un conjunto de datos consolidado a partir de nuestros registros. Este proceso implica un análisis detallado de cada año, con el objetivo de integrar las columnas correspondientes a las observaciones diarias de diversos agentes químicos del aire presentes en cada hoja de Excel. En concreto, disponemos de medidas de Partículas en suspensión (PM10), Partículas en suspensión (PM2,5), Ozono troposférico (O3), Dióxido de nitrógeno (NO2) y Dióxido de azufre (SO2). La finalidad es fusionar estas columnas en una única entidad independiente que contenga todas las observaciones de dicho parámetro durante los cuatro años contemplados en el estudio.

Este enfoque permite la generación de un conjunto de datos estructurado de manera "tidy", donde cada variable se representa en una columna individual y cada observación de una variable ocupa una fila independiente. La consolidación de las diversas tablas y hojas de Excel se lleva a cabo considerando las variables comunes, es decir, los parámetros del aire analizados. Como resultado, nuestro nuevo dataset presenta una primera fila que contiene los nombres representativos de las variables, estableciendo así un marco coherente para el análisis subsiguiente.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(readxl)
library(lubridate)
main_root<-'../data'
carpetas<-c('2018','2019','2020','2021','2022')
subcarpeta1<-'Datos_diarios'
z_f<-list()
n=0
for (c in carpetas){
  files<-dir(paste(main_root,c,subcarpeta1,sep='/'))
  df_t<-list()
  names<-list()
  n=n+1
  i=1
  for (file in files){
    ds1 <- read_delim(paste(main_root,c,subcarpeta1,file,sep='/'), 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
    ds1<-ds1[ds1$PROVINCIA==3|ds1$PROVINCIA==12|ds1$PROVINCIA==46,]
    ds_1t <- pivot_longer(ds1, -(PROVINCIA:DIA),names_to = "HORA", values_to = 'Valor', names_prefix = 'H')
    ds_1t <- ds_1t[, -which(names(ds_1t) == "MAGNITUD")]
    ds_1t <- ds_1t[, -which(names(ds_1t) == "PUNTO_MUESTREO")]
    ds_1t<-mutate(ds_1t,Fecha=ymd(paste(ANNO,MES,DIA,sep='-')))
    ds_1t<-select(ds_1t,-c('ANNO','MES','DIA'))
    ds_1t <- ds_1t %>% 
    group_by(Fecha,MUNICIPIO,PROVINCIA,ESTACION) %>%
    summarise(Valor=max(Valor,na.rm=TRUE))
    new_name=strsplit(file,'_')[[1]][1]
    ds_1t[new_name]<-ds_1t$Valor
    ds_1t<-select(ds_1t,-c('Valor'))
    df_t[[i]]<-ds_1t
    names[[i]]<-new_name
    i=i+1
  }
  names(df_t)<-names
  valids_var<-c('NO2','NOx','O3','PM10','PM25','SO2')
  df_t<-df_t[valids_var]
  z<-merge(df_t[[1]],df_t[[2]],all=TRUE)
  for (i in 3:length(df_t)){
    z<-merge(z,df_t[[i]],all=TRUE)
    
  }
  
z_f[[n]]<-z

}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# METADATOS

# Para 2018
meta_df <- read_excel("../data/2018/Metadatos/metainformacion_2018_tcm30-501408.xlsx", sheet="Estaciones evaluación 2018")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[1]]
z_2018<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2018<-z_2018 %>% select(all_of(columnas))
z_2018<-select(z_2018,-c('PROVINCIA','MUNICIPIO'))

# Para 2019
meta_df <- read_excel("../data/2019/Metadatos/metainformacion_2019_tcm30-513561.xlsx", sheet="Estaciones evaluación 2019")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[2]]
z_2019<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2019<-z_2019 %>% select(all_of(columnas))
z_2019<-select(z_2019,-c('PROVINCIA','MUNICIPIO'))

# Para 2020
meta_df <- read_excel("../data/2020/Metadatos/metainformacionestacionesymagnitudes2020_tcm30-531266.xlsx", 
    sheet = "Estaciones")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[3]]
z_2020<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2020<-z_2020 %>% select(all_of(columnas))
z_2020<-select(z_2020,-c('PROVINCIA','MUNICIPIO'))

# Para 2021
meta_df <- read_excel("../data/2021/Metadatos/metainformacion2021_tcm30-545563.xlsx", sheet="Estaciones")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[4]]
z_2021<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2021<-z_2021 %>% select(all_of(columnas))
z_2021<-select(z_2021,-c('PROVINCIA','MUNICIPIO'))

# Para 2022
meta_df <- read_excel("../data/2022/Metadatos/Metainformacion2022.xlsx", sheet="Estaciones")
meta_df<-meta_df[meta_df$N_RED=='CCAA Com. Valenciana',]
z<-z_f[[5]]
z_2022<-merge.data.frame(meta_df,z, by=c('PROVINCIA','MUNICIPIO','ESTACION'))
columnas<-c('N_PROVINCIA','N_MUNICIPIO','LATITUD_G','LONGITUD_G','TIPO_AREA',colnames(z))
z_2022<-z_2022 %>% select(all_of(columnas))
z_2022<-select(z_2022,-c('PROVINCIA','MUNICIPIO'))
```
    
```{r}
z_final<-rbind(z_2018,z_2019,z_2020,z_2021,z_2022)
#write.csv(z_final, "data/join_datas.csv")
summary(z_final)
```



# Limpieza de datos

Una vez adquirido un conjunto de datos organizado en formato tidy, es necesario llevar a cabo un proceso de limpieza. Este procedimiento implica la evaluación minuciosa de cada inconsistencia, tales como los valores faltantes (NAs) y los valores atípicos (outliers). Con el propósito de abordar estas discrepancias, se implementan diversos procedimientos y técnicas especializadas.

## Outliers

Como es ampliamente conocido, un "outlier" se define como un valor atípico en un conjunto de datos que exhibe una marcada disparidad con respecto al resto de las observaciones. Para analizar la presencia de tales observaciones atípicas, se emplean tres técnicas específicas: la regla de 3 sigma, la regla de Hampel, la regla de boxplot y la regla de los percentiles. Obtuvimos la siguiente tabla:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Método 3-Sigma

library(readr)
df <- read_csv("../data/join_datas.csv") #Hemos guardado el csv con el formato tidy para cargarlo automáticamente y no tener que estas procesando todos los pasos anteriores

find_outliers_3sigma <- function(x) {
  x <- na.omit(x)
  mean_val <- mean(x)
  sd_val <- sd(x)
  lower_bound <- mean_val - 3 * sd_val
  upper_bound <- mean_val + 3 * sd_val
  outliers <- x[x < lower_bound | x > upper_bound]
  positions <- which(x %in% outliers)
  return(list(values = outliers, positions = positions))
}

# Método IQR(Boxplot)

find_outliers_iqr <- function(x) {
  x <- na.omit(x)
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x[x < lower_bound | x > upper_bound]
  positions <- which(x %in% outliers)
  return(list(values = outliers, positions = positions))
}


# Método del Hampel
find_outliers_hampel <- function(df_column, k = 3) {
  x <- na.omit(df_column)
  med <- median(x)
  mad <- median(abs(x - med))
  threshold <- k * 1.4826 * mad
  outliers <- x[abs(x - med) > threshold]
  positions <- which(df_column %in% outliers)
  return(list(values = outliers, positions = positions))
}

# Método de los percentiles
find_outliers_percentile <- function(x) {
  x <- na.omit(x)
  lower_bound <- quantile(x, 0.05)
  upper_bound<- quantile(x, 0.95)
  outliers <- x[x < lower_bound | x > upper_bound]
  positions <- which(x %in% outliers)
  return(list(values = outliers, positions = positions))
}
```


```{r echo=FALSE, message=FALSE, warning=FALSE}

# Aplicamos a nuestros datos los métodos anteriores

columnas_analizar <- c("NO2", "NOx", "O3", "PM10", "PM25", "SO2")

outliers_3sigma_list <- lapply(df[columnas_analizar], find_outliers_3sigma)
outliers_hampel_list <- lapply(df[columnas_analizar], find_outliers_hampel)
outliers_iqr_list <- lapply(df[columnas_analizar], find_outliers_iqr)
outliers_percentile_list <- lapply(df[columnas_analizar], find_outliers_percentile)

#print("Outliers detectados usando la regla 3-sigma:")
#print(outliers_3sigma_list)

#print("Outliers detected using Hampel identifier:")
#print(outliers_hampel_list)

#print("Outliers detected using IQR(Boxplot):")
#print(outliers_iqr_list)

#print("Outliers detected using IQR:")
#print(outliers_percentile_list)

```


Aquí utilizamos los códigos implementados anteriormente en cada una de las variables numéricas para detectar los outliers

```{r}

outliers_summary <- data.frame(Chemical_Element = rep(columns_to_analyze, each = 4),
                                Method = rep(c("3-Sigma", "IQR", "Hampel", "Percentil"), times = length(columns_to_analyze)),
                                Outliers_Count = c(
                                  sapply(outliers_3sigma_list, function(x) length(x$values)),
                                  sapply(outliers_iqr_list, function(x) length(x$values)),
                                  sapply(outliers_hampel_list, function(x) length(x$values)),
                                  sapply(outliers_percentile_list, function(x) length(x$values))
                                ))

print(outliers_summary)

```



```{r}
library(tidyverse)

outliers_summary_transformed <- outliers_summary %>%
  pivot_wider(names_from = Method, values_from = Outliers_Count)

print(outliers_summary_transformed)
```

En esta tabla visualizamos un recuento de la cantidad de outliers detectados con cada método. Ahora bien, tenemos que analizar que consideramos un valor atípico en nuestro caso. Para ello, debemos establecer algun criterio. Después de documentarnos, encontramos en el BOE (Boletín oficial del Estado) la siguiente tabla:


| $S0_{2}$ | PM2,5 | PM10 | $O_{3}$ | $NO_{2}$ | CATEGORÍA DEL ÍNDICE |
|--------|-------|------|-------|--------|----------------------|
| 751-1250| 76-800| 151-1200| 381-800 | 341-1000 | Extremadamente Desfavorable|


Para utilizar la detección de valores atípicos y asignarles esta categoría, en nuestro caso calculamos tanto el valor máximo como el mínimo para cada uno de los parámetros químicos. Este proceso nos permite evaluar si dichos valores caen fuera del rango establecido en la tabla anterior, lo que justificaría su clasificación como outliers. Tras realizar estos cálculos, hemos verificado que los valores identificados como outliers son válidos y, por ende, hemos decidido mantenerlos en el conjunto de datos.


```{r, echo=FALSE}
(max_values <- lapply(outliers_3sigma_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_3sigma_list, function(x) min(x$values, na.rm = TRUE)))

(max_values <- lapply(outliers_hampel_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_hampel_list, function(x) min(x$values, na.rm = TRUE)))

(max_values <- lapply(outliers_iqr_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_iqr_list, function(x) min(x$values, na.rm = TRUE)))

(max_values <- lapply(outliers_percentile_list, function(x) max(x$values, na.rm = TRUE)))
(min_values <- lapply(outliers_percentile_list, function(x) min(x$values, na.rm = TRUE)))
```

## Imputación NAs

El siguiente paso en el proceso de limpieza de nuestro conjunto de datos implica la imputación de los valores faltantes, comúnmente conocidos como NAs. Para abordar esta tarea, hemos estudiado diversas técnicas en clase, entre las cuales se incluyen: la eliminación de datos, la imputación con un valor estimado basado en los valores conocidos, la imputación mediante un modelo predictivo y la imputación al valor más cercano utilizando el algoritmo KNN (k-vecinos más cercanos). Se recomienda preferentemente emplear alguna de las dos últimas opciones, reservando las opciones de eliminación de datos únicamente cuando no existan alternativas viables.

Como consecuencia, inicialmente decidimos emprender el proceso de imputación mediante la aplicación de un método predictivo. Implementamos un modelo de regresión lineal específicamente para las variables NOx y NO2. No obstante, es importante señalar que el éxito de esta metodología fue limitado, logrando imputar únicamente tres datos.

```{r, echo=FALSE}
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
train  <- df[sample, ]
test   <- df[!sample, ]

lm.NO2<-lm(NO2~NOx,train)
summary(lm.NO2)

predict<-predict.lm(lm.NO2,test)
RMSE<-sqrt(mean((test$NO2-predict) ^ 2, na.rm=T))
SD_NO2<-sd(df$NO2,na.rm=T)

hist(df$NO2)
summary(df$NO2)
```
 
 
    
```{r, echo=FALSE}
df_imputed1<-df
df_imputed1[is.na(df_imputed1$NO2),]$NO2<-predict.lm(lm.NO2,df_imputed1[is.na(df_imputed1$NO2),])
hist(df_imputed1$NO2)
summary(df_imputed1$NO2)
```





# Análisis Univariante


En esta sección, procederemos a llevar a cabo un análisis univariante sobre el conjunto de datos previamente procesado. El propósito principal de esta evaluación consiste en examinar individualmente cada variable presente en el dataset, con el objetivo de obtener un panorama detallado de su distribución y variabilidad.

En primer lugar, para tener una visión global de este dataset sacamos un resumen estadístico

```{r}
library(dplyr)
library(readr)

data <- read_csv("../data/imputed_datas.csv")
```

```{r}
# Resumen estadístico
summary(data)

```


Una vez obtenidos los estadísticos. Visualizamos nuestras variables.


```{r, echo=FALSE}
library(ggplot2)
library(grid)

create_histogram <- function(data, contaminante, x_label) {
  ggplot(data, aes(x = get(paste0(contaminante, ".num")))) +
    geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
    labs(title = paste("Distribución de", contaminante), x = x_label) +
    theme_minimal()
}

graficas <- lapply(c("NO2", "O3", "PM10", "PM25", "SO2"), function(contaminante) {
  create_histogram(data, contaminante, paste("Concentración de", contaminante))
})

# Combinamos las gráficas en una única representación
multiplot <- function(..., plotlist = NULL, cols = 1, layout = NULL) {
  require(grid)

  
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  
  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }

  if (numPlots == 1) {
    print(plots[[1]])

  } else {
    
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    for (i in 1:numPlots) {
      
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

multiplot(plotlist = graficas, cols = 2)

```

La representación gráfica previa proporciona una visión general de la concentración de cada elemento en nuestro conjunto de datos. La representación más notable en términos de su estructura y forma corresponde al elemento $O_{3}$, ya que parece seguir una distribución gaussiana. Los datos están distribuidos de manera uniforme a lo largo del eje x, que representa la concentración, y muestran una distribución alrededor de la media y la mediana, ambas con valores muy similares, como se confirmó en los estadísticos previamente descritos. (88.60 y 88.00 respectivamente).

En contraste, las representaciones de los otros elementos exhiben una asimetría significativa, con valores relativamente altos al principio seguidos de una disminución. Estas observaciones son respaldadas numéricamente por los estadísticos calculados anteriormente. Por ejemplo, al considerar el elemento $S0_{2}$, se observa un valor máximo de 239,000. Sin embargo, la media calculada es de 6,359, un valor considerablemente distante del máximo mencionado. Además, el valor mínimo es 1, mientras que la mediana es 4,000. Estas discrepancias justifican la representación gráfica de esta información. Esto mismo sucedería con los elementos restantes.


A continuación, procederemos a visualizar la evolución de los compuestos químicos a lo largo del tiempo, con el objetivo de realizar un análisis más exhaustivo sobre la calidad del aire.


```{r}
library(ggplot2)

# Crear gráficos de densidad
ggplot(data, aes(x = NO2.num, fill = "NO2")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de NO2", x = "Concentración de NO2")

ggplot(data, aes(x = O3.num, fill = "O3")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de O3", x = "Concentración de O3")

ggplot(data, aes(x = PM10.num, fill = "PM10")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de PM10", x = "Concentración de PM10")

ggplot(data, aes(x = PM25.num, fill = "PM2.5")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de PM2.5", x = "Concentración de PM2.5")

ggplot(data, aes(x = SO2.num, fill = "SO2")) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de SO2", x = "Concentración de SO2")

ggplot(data, aes(x = Dias)) +
  geom_density(alpha = 0.5) +
  labs(title = "Densidad de Días", x = "Número de Días")

```


```{r}
library(tidyverse)
library(ggplot2)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  group_by(year = lubridate::year(Fecha)) %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de líneas superpuestas con datos escalados y facet_wrap por año
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled, color = "NO2")) +
  geom_line(aes(y = O3_scaled, color = "O3")) +
  geom_line(aes(y = PM10_scaled, color = "PM10")) +
  geom_line(aes(y = PM25_scaled, color = "PM25")) +
  geom_line(aes(y = SO2_scaled, color = "SO2")) +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Años",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal() +
  facet_wrap(~year, scales = "free_y", ncol = 4)



```


```{r}
library(tidyverse)
library(ggplot2)

# Convertir Fecha a formato de fecha
data$Fecha <- as.Date(data$Fecha)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  group_by(year = lubridate::year(Fecha)) %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de líneas superpuestas con datos escalados, facet_wrap por año y suavizado por media mensual
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled, color = "NO2")) +
  geom_line(aes(y = O3_scaled, color = "O3")) +
  geom_line(aes(y = PM10_scaled, color = "PM10")) +
  geom_line(aes(y = PM25_scaled, color = "PM25")) +
  geom_line(aes(y = SO2_scaled, color = "SO2")) +
  geom_smooth(aes(y = NO2_scaled), method = "loess", se = FALSE, color = "red") +
  geom_smooth(aes(y = O3_scaled), method = "loess", se = FALSE, color = "blue") +
  geom_smooth(aes(y = PM10_scaled), method = "loess", se = FALSE, color = "green") +
  geom_smooth(aes(y = PM25_scaled), method = "loess", se = FALSE, color = "purple") +
  geom_smooth(aes(y = SO2_scaled), method = "loess", se = FALSE, color = "orange") +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Años",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal() +
  facet_wrap(~year, scales = "free_y", ncol = 4)

```


```{r}
library(tidyverse)
library(ggplot2)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  group_by(year = lubridate::year(Fecha)) %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de puntos superpuestos con datos escalados y facet_wrap por año
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled, color = "NO2"), size = 2, alpha = 0.7) +
  geom_line(aes(y = O3_scaled, color = "O3"), size = 2, alpha = 0.7) +
  geom_line(aes(y = PM10_scaled, color = "PM10"), size = 2, alpha = 0.7) +
  geom_line(aes(y = PM25_scaled, color = "PM25"), size = 2, alpha = 0.7) +
  geom_line(aes(y = SO2_scaled, color = "SO2"), size = 2, alpha = 0.7) +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Años",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal() +
  facet_wrap(~year, scales = "free_y", ncol = 4)

```

```{r}
library(tidyverse)
library(ggplot2)

# Convertir Fecha a formato de fecha
data$Fecha <- as.Date(data$Fecha)

# Aplicar la estandarización min-max a los datos por año
data_scaled <- data %>%
  mutate(
    NO2_scaled = (NO2.num - min(NO2.num)) / (max(NO2.num) - min(NO2.num)),
    O3_scaled = (O3.num - min(O3.num)) / (max(O3.num) - min(O3.num)),
    PM10_scaled = (PM10.num - min(PM10.num)) / (max(PM10.num) - min(PM10.num)),
    PM25_scaled = (PM25.num - min(PM25.num)) / (max(PM25.num) - min(PM25.num)),
    SO2_scaled = (SO2.num - min(SO2.num)) / (max(SO2.num) - min(SO2.num))
  )

# Crear un gráfico de líneas superpuestas con datos escalados y suavizado por media mensual
ggplot(data_scaled, aes(x = Fecha, color = "Compuesto")) +
  geom_line(aes(y = NO2_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = O3_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = PM10_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = PM25_scaled), alpha = 0.5, size = 1) +
  geom_line(aes(y = SO2_scaled), alpha = 0.5, size = 1) +
  geom_smooth(aes(y = NO2_scaled), method = "loess", se = FALSE, color = "red") +
  geom_smooth(aes(y = O3_scaled), method = "loess", se = FALSE, color = "blue") +
  geom_smooth(aes(y = PM10_scaled), method = "loess", se = FALSE, color = "green") +
  geom_smooth(aes(y = PM25_scaled), method = "loess", se = FALSE, color = "purple") +
  geom_smooth(aes(y = SO2_scaled), method = "loess", se = FALSE, color = "orange") +
  labs(title = "Concentración de Compuestos Químicos a lo largo del tiempo (Escala min-max)",
       x = "Fecha",
       y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal()


```

```{r}
library(tidyverse)
library(ggplot2)

# Cargar tus datos
# Por ejemplo, asumiendo que tus datos se llaman "data"
# data <- read.csv("tu_archivo.csv")

# Crear un gráfico de densidad superpuestas
ggplot(data, aes(x = NO2.num, fill = "NO2")) +
  geom_density(alpha = 0.5) +
  geom_density(aes(x = O3.num, fill = "O3"), alpha = 0.5) +
  geom_density(aes(x = PM10.num, fill = "PM10"), alpha = 0.5) +
  geom_density(aes(x = PM25.num, fill = "PM25"), alpha = 0.5) +
  geom_density(aes(x = SO2.num, fill = "SO2"), alpha = 0.5) +
  labs(title = "Densidad de Compuestos Químicos",
       x = "Concentración",
       y = "Densidad") +
  scale_fill_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  theme_minimal()

```


```{r}
# Tendencia temporal de NO2.num
ggplot(data, aes(x =Fecha, y = NO2.num)) +
  geom_line() +
  labs(title = "Tendencia Temporal de NO2.num", x = "Fecha", y = "Valor")

```






Pensar que hacer con los nombres

```{r}
# Cargar biblioteca ggplot2
library(ggplot2)

# Crear un gráfico de barras apiladas con etiquetas del eje x rotadas
ggplot(data, aes(x = N_MUNICIPIO, fill = N_PROVINCIA)) +
  geom_bar(position = "stack") +
  labs(title = "Frecuencias de Categorías", x = "Tipo de Área") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```



```{r}
# Cargar bibliotecas necesarias
library(ggplot2)

# Crear una columna 'variable' con los nombres de las variables
data$variable <- rep(c("NO2", "O3", "PM10", "PM25", "SO2"), length.out = nrow(data))

# Crear un gráfico de barras con facet_wrap
ggplot(data, aes(x = N_PROVINCIA, fill = variable)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(title = "Relación entre Elementos Químicos y Provincias", x = "Provincia", y = "Frecuencia") +
  scale_fill_manual(values = c("NO2" = "red", "O3" = "blue", "PM10" = "green", "PM25" = "purple", "SO2" = "orange")) +
  facet_wrap(~variable, scales = "free_y", ncol = 1) +
  theme_minimal()


```


```{r warning=TRUE}
library(zoo)
data$Fecha <- as.Date(data$Fecha)
(data.zoo <- zoo(data[, -c(1:3, 13:15)], order.by = data$Fecha))

```













# Análisis Bivariante

Para realizar el análisis bivariante primer observaremos como se relacionan todas las variables entre sí

Primero vamos a realizar un `ggpairs` y un `ggcorr`para relacionar todos los elementos químicos con 
todos (valores numéricos)

```{r}

library(GGally)
num_cols <- c("NO2.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")
ggpairs(df[num_cols])
```

 

```{r}
ggcorr(df[num_cols])
```
Como podemos ver, la relación de las variables no es clara. De forma general podemos decir que las variables
son bastante independientes entre sí.Como excepción podríamos hablar de las variables PM10 y PM25

```{r}
Cov_Mat <- cov(df[num_cols])

print("Matriz de covarianza")
print(Cov_Mat)

```


```{r}
Cor_Mat_S <- cor(df[num_cols], method = "spearman")
Cor_Mat_P <- cor(df[num_cols], method = "pearson")

print("Matriz de correlacion (Spearman)")
print(Cor_Mat_S)
print("Matriz de correlacion (Pearson)")
print(Cor_Mat_P)

```

Ya que hemos visto que las variables PM10 y PM25, vamos a relacionar sus variables categóricas pra ver la relación.


```{r}

chisq.test(df$PM10.cat, df$PM25.cat, correct = F)

```

Podemos ver que el p-valor es muy pequeño, luego podemos rechazar la hipótesis de que no están relacionadas.
Este efecto es más grande en las categóricas, ya que "medimos" por intervalos




## NO2

```{r}
# Gráfico de dispersión entre NO2.num y O3.num
ggplot(data, aes(x = NO2.num, y = O3.num)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot entre NO2.num y O3.num", x = "NO2.num", y = "O3.num")

```

```{r}
# Matriz de correlación
cor(data[, c("NO2.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")])

```


```{r}
# Gráfico de barras agrupadas para la variable TIPO_AREA y NO2.num
ggplot(data, aes(x = TIPO_AREA, y = NO2.num, fill = TIPO_AREA)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(title = "Promedio de NO2.num por TIPO_AREA", x = "TIPO_AREA", y = "Promedio NO2.num")

```


```{r}
# Mapa de calor de la matriz de correlación
cor_matrix <- cor(data[, c("NO2.num", "O3.num", "PM10.num", "PM25.num", "SO2.num")])
heatmap(cor_matrix, col = colorRampPalette(c("blue", "white", "red"))(50), 
        main = "Mapa de Calor de Correlación")

```



```{r}
# Gráfico de líneas para NO2.num y O3.num a lo largo del tiempo
ggplot(data, aes(x = Fecha)) +
  geom_line(aes(y = NO2.num, color = "NO2")) +
  geom_line(aes(y = O3.num, color = "O3")) +
  labs(title = "Evolución de NO2.num y O3.num a lo largo del tiempo", x = "Fecha", y = "Concentración") +
  scale_color_manual(values = c("NO2" = "red", "O3" = "blue"))

```



```{r}
# Boxplot para comparar la distribución de NO2.num en diferentes estaciones
ggplot(data, aes(x = year, y = NO2.num, fill = ESTACION)) +
  geom_boxplot() +
  labs(title = "Distribución de NO2.num por Estación", x = "Estación", y = "NO2.num")

```

```{r}
# Matriz de gráficos de dispersión para varias variables
pairs(~NO2.num + O3.num + PM10.num + PM25.num + SO2.num, data = data)

```




```{r}
# Cargar datos geoespaciales de países (usaremos 'world' en lugar de 'ne_countries')
world <- ne_download(scale = 110, returnclass = "sf")

# Crear un objeto sf con las coordenadas de tu conjunto de datos
data_sf <- st_as_sf(data, coords = c("X_UTM30N", "Y_UTM30N"), crs = st_crs(world))

# Crear el mapa
ggplot() +
  geom_sf(data = world) +
  geom_sf(data = data_sf, color = "red", size = 2) +
  labs(title = "Ubicación de las coordenadas en España") +
  theme_minimal()








```






```{r}
# # Instalar y cargar las librerías
# library(sf)
# library(leaflet)
# 
# # Cargar datos desde el archivo JSON
# ruta_al_archivo <- "../data/CV-MAP.json"
# datos_sf <- st_read(ruta_al_archivo)
# 
# # Crear un mapa interactivo con Leaflet
# mapa <- leaflet() %>%
#   addTiles() %>%
#   addMarkers(data = datos_sf, ~X_UTM30N, ~Y_UTM30N, popup = ~as.character(geometry))
# 
# # Mostrar el mapa
# mapa

```

# Conclusiones











































# Results

This section may be divided by subheadings. It should provide a concise and
precise description of the experimental results, their interpretation as well
as the experimental conclusions that can be drawn.

## Subsection Heading Here

Subsection text here.

### Subsubsection Heading Here

Bulleted lists look like this:

* First bullet
* Second bullet
* Third bullet

Numbered lists can be added as follows:

1. First item
2. Second item
3. Third item

The text continues here.

## Figures, Tables and Schemes

All figures and tables should be cited in the main text as Figure \ref{fig:fig1}, 
\ref{tab:tab1}, etc. To get cross-reference to figure generated by R chunks 
include the `\\label{}` tag in the `fig.cap` attribute of the R chunk: 
`fig.cap = "Fancy Caption\\label{fig:plot}"`.

```{r fig1, echo=FALSE, fig.width=3, fig.cap="A figure added with a code chunk.\\label{fig:fig1}"}
x = rnorm(10)
y = rnorm(10)
plot(x, y)
```


When making tables using `kable`, it is suggested to use
the `format="latex"` and `tabl.envir="table"` arguments
to ensure table numbering and compatibility with the mdpi
document class.

```{r tab1, echo=FALSE}
knitr::kable(mtcars[1:5, 1:3], format = "latex", 
             booktabs = TRUE, 
             caption = "This is a table caption. Tables should be placed in the 
             main text near to the first time they are~cited.", 
             align = 'ccc', centering = FALSE,
             table.envir = "table", position = "H")
```



For a very wide table, landscape layouts are allowed.


\startlandscape

```{r tab2, echo=FALSE}
df <- data.frame(`Title 1` = c("Entry 1", "Entry 2"),
                 `Title 2` = c("Data", "Data"),
                 `Title 3` = c("Data", "Data"),
                 `Title 4` = c("This cell has some longer content that runs over
                               two lines",
                               "Data"))
knitr::kable(df, format = "latex", 
             booktabs = TRUE, 
             caption = "This is a very wide table", 
             align = 'ccc', centering = FALSE,
             table.envir = "table", position = "H")
```

\finishlandscape

## Formatting of Mathematical Components

This is an example of an equation:

$$
a = 1.
$$

If you want numbered equations use Latex and wrap in the equation environment:

\begin{equation}
a = 1,
\end{equation}

the text following an equation need not be a new paragraph. Please punctuate 
equations as regular text.

```{comment}
If the documentclass option "submit" is chosen, please insert a blank line before and after any math environment (equation and eqnarray environments). This ensures correct linenumbering. The blank line should be removed when the documentclass option is changed to "accept" because the text following an equation should not be a new paragraph.
```

This is the example 2 of equation:

\begin{adjustwidth}{-\extralength}{0cm}
\begin{equation}
a = b + c + d + e + f + g + h + i + j + k + l + m + n + o + p + q + r + s + t + 
u + v + w + x + y + z
\end{equation}
\end{adjustwidth}

Theorem-type environments (including propositions, lemmas, corollaries etc.) 
can be formatted as follows:

Example of a theorem:

::: {.Theorem latex=true}
Example text of a theorem
:::

The text continues here. Proofs must be formatted as follows:

Example of a proof:

::: {.proof latex="[Proof of Theorem1]"}
Text of the proof. Note that the phrase ``of Theorem 1'' is optional if it is 
clear which theorem is being referred to.
:::

The text continues here.

# Discussion

Authors should discuss the results and how they can be interpreted in 
perspective of previous studies and of the working hypotheses. The findings and 
their implications should be discussed in the broadest context possible. Future 
research directions may also be highlighted.

# Conclusion

This section is not mandatory, but can be added to the manuscript if the
discussion is unusually long or complex.

# Patents

This section is not mandatory, but may be added if there are patents resulting
from the work reported in this manuscript.
